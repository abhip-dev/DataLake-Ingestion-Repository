{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-01T06:13:24.184773Z",
     "start_time": "2019-01-01T06:13:23.741063Z"
    }
   },
   "outputs": [],
   "source": [
    "## import the necessaries\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import keyring\n",
    "import getpass\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "from impala.dbapi import connect\n",
    "import subprocess\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy.pool import NullPool\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-01T06:13:26.485753Z",
     "start_time": "2019-01-01T06:13:26.467755Z"
    }
   },
   "outputs": [],
   "source": [
    "# make impyla connection to run action queries thru impala interface\n",
    "def impala_conn():\n",
    "    return connect(host='poldcdhdn010.dev.intranet', \n",
    "                   port=21050,\n",
    "                   use_ssl=True,\n",
    "                   auth_mechanism='GSSAPI')\n",
    "impala = create_engine('impala://', creator=impala_conn, poolclass=NullPool) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Pysqoop function - altered from Stephen Quinn's original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-01-01T06:13:28.955259Z",
     "start_time": "2019-01-01T06:13:28.698783Z"
    }
   },
   "outputs": [],
   "source": [
    "# sqoops oracle table into CDL. Expected params: tablename,schema, hdfs_dir\n",
    "# example: tableaname='motive68', schema='bdalab_sa', \n",
    "# pysqoop(tablename=tablename, schema=schema, hdfs_dir=hdfs_dir)\n",
    "# Optional param: merge_key (string, used like a primary key for upserts, oracle_query\n",
    "# Optional params: oracle_query (select statement from oracle. only supports one where clause)\n",
    "def pysqoop(**kwargs):\n",
    "    '''\n",
    "    This takes an oracle query and sqoops the result into the dev DataLake. \n",
    "    keyword args:\n",
    "        tablename - string - name of table in Oracle and in the datalake\n",
    "        schema - string - schema for the datalake table being created/inserted into\n",
    "        oracle_schema - string - schema, if different, for the oracle table being sqooped\n",
    "        hdfs_dir - \n",
    "    '''\n",
    "    tablename = kwargs.get('tablename')\n",
    "    schema = kwargs.get('schema')\n",
    "    oracle_schema = kwargs.get('oracle_schema','')\n",
    "    hdfs_dir = kwargs.get('hdfs_dir', '/data/CTL/encrypt/db/')\n",
    "    merge_key = kwargs.get('merge_key', '')\n",
    "    is_impala = kwargs.get('is_impala', False)\n",
    "    is_reload = kwargs.get('is_reload',False)\n",
    "    # to do - this would be good to give some of this as a list from our json file at some point\n",
    "    oracle_host = kwargs.get('oracle_host','pddclodm-scan.corp.intranet')\n",
    "    oracle_port = kwargs.get('oracle_port','1521')\n",
    "    oracle_service = kwargs.get('oracle_service','CCDW01PU1')\n",
    "    oracle_query = kwargs.get('oracle_query')\n",
    "    split_column = kwargs.get('splitby',None)\n",
    "    usr = kwargs.get('usr',getpass.getuser())\n",
    "    pwd = kwargs.get('pwd',keyring.get_password('CCDW01PU1', usr))\n",
    "\n",
    "#     for key in kwargs:\n",
    "#         print(kwargs[key])\n",
    "    if oracle_schema and not oracle_query:\n",
    "        oracle_query= \"SELECT * FROM {}.{}\".format(oracle_schema,tablename)\n",
    "    elif not oracle_query:\n",
    "        oracle_query = \"SELECT * FROM {}.{}\".format(usr,tablename)\n",
    "    # add oracle schema to tablename for DL if present\n",
    "    if oracle_schema:\n",
    "        tablename = \"{}_{}\".format(oracle_schema,tablename)\n",
    "        \n",
    "    target_path =  '{}{}/{}_stage'.format(hdfs_dir, schema, tablename)\n",
    "    target_table = '{}.{}'.format(schema,tablename)\n",
    "    stage_path = '{}{}/{}_temp'.format(hdfs_dir, schema, tablename)\n",
    "    stage_table = target_table+'_work'\n",
    "    # clean data and format\n",
    "    oracle_path = 'jdbc:oracle:thin:@//{}:{}/{}'.format(oracle_host,oracle_port,oracle_service)\n",
    "    if '$CONDITIONS' not in oracle_query:\n",
    "        if 'where' in oracle_query.lower():\n",
    "            oracle_query = '\"' + oracle_query + ' and \\$CONDITIONS\"'\n",
    "        else:\n",
    "            oracle_query = '\"' + oracle_query + ' where \\$CONDITIONS\"'\n",
    "    oracle_query = oracle_query.replace('\\n', ' ').replace('\\r', '').replace('\"','')   \n",
    "    print(oracle_query)\n",
    "    # drop stage table if it exists\n",
    "    \n",
    "    invalidate_meta = \"INVALIDATE METADATA {}\"\n",
    "    refresh_table = \"REFRESH {}\"\n",
    "    drop_table = \"drop table if exists {} purge\"\n",
    "    \n",
    "    if is_impala:\n",
    "        try:\n",
    "            impala_engine = create_engine('impala://', creator=impala_conn, poolclass=NullPool)\n",
    "            impala = impala_engine # to-do: replace impala_engine with impala\n",
    "            cdl_tables = pd.read_sql(\"show tables in {}\".format(schema), impala_engine).name.tolist()\n",
    "            try:\n",
    "                impala.execute(invalidate_meta.format(stage_table))\n",
    "                impala.execute(refresh_table.format(stage_table))\n",
    "                impala.execute(drop_table.format(stage_table))\n",
    "            except Exception as e:\n",
    "#                 print(\"try 2.0 failed\",e)\n",
    "                pass\n",
    "        except Exception as e: # if impala doesn't work, switch to hive/spark\n",
    "#             print(\"try 1.0 failed\",e)\n",
    "            try:\n",
    "                print('trying spark/hive...')\n",
    "                #is_impala = False\n",
    "                cdl_tables = sqlContext.sql(\"show tables in {}\".format(schema)).toPandas()\n",
    "                cdl_tables = cdl_tables.tableName.tolist()\n",
    "                sqlContext.sql(drop_table.format(stage_table))\n",
    "            except Exception as e:\n",
    "                #print(e)\n",
    "#                 print(\"try 1 then 2.5 failed\",e)\n",
    "                pass\n",
    "    else:\n",
    "        print('is_impala=False, trying spark/hive...')\n",
    "        cdl_tables = sqlContext.sql(\"show tables in {}\".format(schema)).toPandas()\n",
    "        cdl_tables = cdl_tables.tableName.tolist()\n",
    "        sqlContext.sql(drop_table.format(stage_table))\n",
    "    \n",
    "    \"\"\"\n",
    "    try:\n",
    "        if is_impala:\n",
    "            try:\n",
    "                try:\n",
    "                    impala.execute(invalidate_meta.format(stage_table))\n",
    "                    impala.execute(refresh_table.format(stage_table))\n",
    "                except:\n",
    "                    pass\n",
    "                impala.execute(drop_table.format(stage_table))\n",
    "                cdl_tables = pd.read_sql(\"show tables in {}\".format(schema), impala_engine).name.tolist()\n",
    "            except Exception as e: # if impala doesn't work, switch to hive/spark\n",
    "                print(e)\n",
    "                is_impala = False\n",
    "                sqlContext.sql(drop_table.format(stage_table))\n",
    "        else:\n",
    "            sqlContext.sql(\"drop table if exists {} purge\".format(stage_table))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Setup Scoop\n",
    "    if split_column:\n",
    "        sqoop_this =r'''sqoop import --connect {0} --username {1} --password {2} --as-textfile --query \"{3}\" --target-dir {4} --delete-target-dir --fields-terminated-by '\\0x7C' --lines-terminated-by '\\n' --null-string '\\\\N' --null-non-string '\\\\N' --split-by {5} --num-mappers 4 --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --hive-import --hive-drop-import-delims --create-hive-table -hive-table {6} --verbose;'''.format(oracle_path,usr,pwd,oracle_query,stage_path,split_column,stage_table)\n",
    "    else:\n",
    "        sqoop_this =r'''sqoop import --connect {0} --username {1} --password {2} --as-textfile --query \"{3}\" --target-dir {4} --delete-target-dir --fields-terminated-by '\\0x7C' --lines-terminated-by '\\n' --null-string '\\\\N' --null-non-string '\\\\N' -m 1 --compress --compression-codec org.apache.hadoop.io.compress.SnappyCodec --hive-import --hive-drop-import-delims --create-hive-table -hive-table {5} --verbose;'''.format(oracle_path,usr,pwd,oracle_query,stage_path,stage_table)\n",
    "    r = os.system(sqoop_this)\n",
    "    \n",
    "    \n",
    "    try:\n",
    "        if r == 0:\n",
    "            print('Sqooping was successful. Processing {} table in the datalake'.format(stage_table))\n",
    "            \n",
    "            # set up queries\n",
    "            if merge_key:\n",
    "                merge_query = \"\"\"\n",
    "                        INSERT INTO {0}\n",
    "                        SELECT DISTINCT *\n",
    "                        FROM\n",
    "                          ( SELECT *\n",
    "                            FROM {1}\n",
    "                            WHERE {2} NOT IN\n",
    "                               (SELECT {2}\n",
    "                                FROM {0}) \n",
    "                         ) WORK\n",
    "                \"\"\".format(target_table,stage_table, merge_key)\n",
    "            else:\n",
    "                merge_query = \"\"\" SELECT DISTINCT * FROM {0} \"\"\".format(stage_table)     \n",
    "        \n",
    "            create_query = \"\"\"CREATE TABLE {0} stored as parquet as SELECT DISTINCT * FROM {1}\"\"\".format(target_table,stage_table)\n",
    "            #impala.execute('invalidate metadata {}'.format(stage_table))\n",
    "            if tablename.lower() not in cdl_tables:\n",
    "                print('creating table {}'.format(target_table))\n",
    "                if is_impala:\n",
    "                    try:\n",
    "                        impala.execute(invalidate_meta.format(stage_table))\n",
    "                        impala.execute(refresh_table.format(stage_table))\n",
    "                        impala.execute(create_query)\n",
    "                    except Exception as e: # if impala doesn't work, switch to hive/spark\n",
    "                        print(e)\n",
    "                        print('trying with spark/hive...')\n",
    "                        #is_impala = False\n",
    "                        sqlContext.sql(create_query)\n",
    "                else:\n",
    "                    sqlContext.sql(create_query)\n",
    "            elif is_reload: # this allows us to completely replace an existing table, based on the create_query\n",
    "                print('dropping old version of table for reload...')\n",
    "                if is_impala:\n",
    "                    try:\n",
    "                        impala.execute(drop_table.format(target_table))\n",
    "                        impala.execute(invalidate_meta.format(stage_table))\n",
    "                        impala.execute(refresh_table.format(stage_table))\n",
    "                        impala.execute(create_query)\n",
    "                    except Exception as e: # if impala doesn't work, switch to hive/spark\n",
    "                        print(e)\n",
    "                        print('trying with spark/hive...')\n",
    "                        #is_impala = False\n",
    "                        sqlContext.sql(drop_table.format(target_table))\n",
    "                        sqlContext.sql(create_query)\n",
    "                else:\n",
    "                    sqlContext.sql(drop_table.format(target_table))\n",
    "                    sqlContext.sql(create_query)\n",
    "            else:\n",
    "                print('inserting into table {}'.format(target_table))\n",
    "                if is_impala:\n",
    "                    try:\n",
    "                        impala.execute(invalidate_meta.format(stage_table))\n",
    "                        impala.execute(refresh_table.format(stage_table))\n",
    "                        impala.execute(merge_query)\n",
    "                    except Exception as e: # if impala doesn't work, switch to hive/spark\n",
    "                        print(e)\n",
    "                        print('trying with spark/hive...')\n",
    "                        #is_impala = False\n",
    "                        sqlContext.sql(merge_query)\n",
    "                else:\n",
    "                    sqlContext.sql(merge_query)\n",
    "            \n",
    "            if is_impala:\n",
    "                impala.execute(\"refresh {}\".format(target_table))\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        sqlContext.sql(\"drop table if exists {} purge\".format(stage_table))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup table kwargs, then run thru Pysqoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-22T01:24:00.997102Z",
     "start_time": "2018-12-22T01:23:21.090730Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/CTL/encrypt/db/\n",
      "DWBS01PU1\n",
      "cds_contact_medium_profile_ref\n",
      "stage\n",
      "hcoWxTkr2gWt7c2K1lplHWqIXpk5V\n",
      "sarabot\n",
      "True\n",
      "True\n",
      "bdalab_sa\n",
      "SELECT * FROM stage.cds_contact_medium_profile_ref where \\$CONDITIONS\n",
      "try 2.0 failed (impala.error.HiveServer2Error) TableNotFoundException: Table not found: bdalab_sa.stage_cds_contact_medium_profile_ref_work\n",
      " [SQL: 'INVALIDATE METADATA bdalab_sa.stage_cds_contact_medium_profile_ref_work']\n",
      "Sqooping was successful. Processing bdalab_sa.stage_cds_contact_medium_profile_ref_work table in the datalake\n",
      "dropping old version of table for reload...\n"
     ]
    }
   ],
   "source": [
    " # tablename is preset in Setup Files. Overwrite if needed.\n",
    "oracle_schema = \"stage\"\n",
    "oracle_service = \"DWBS01PU1\"\n",
    "schema = 'bdalab_sa'\n",
    "hdfs_dir = '/data/CTL/encrypt/db/'\n",
    "usr='sarabot'\n",
    "my_tables = [\"CDS_CONTACT_MEDIUM_PROFILE\".lower(),\"cds_customer_account\"] # ,\"cds_contact_medium_profile_ref\"\n",
    "for tablename in my_tables:\n",
    "    oracle_query = \"select * from {}.{}\".format(oracle_schema,tablename)\n",
    "    try:\n",
    "        pysqoop(tablename=tablename, \n",
    "                oracle_schema=oracle_schema,\n",
    "                oracle_service=oracle_service,\n",
    "                schema=schema, \n",
    "                hdfs_dir=hdfs_dir,\n",
    "                is_impala=True,\n",
    "                is_reload=True,\n",
    "                usr=usr,\n",
    "                pwd=keyring.get_password(oracle_service, usr)\n",
    "                )\n",
    "    except Exception as e:\n",
    "        print(e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2.7 PySpark 2.1",
   "language": "python",
   "name": "pyspark2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
